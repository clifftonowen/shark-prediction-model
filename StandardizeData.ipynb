{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a20d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6098056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_shark_data(input_filename='shark_data.csv', output_filename='standardized_shark_data_daily.csv'):\n",
    "    \"\"\"\n",
    "    Loads raw shark tracking data, standardizes it by calculating the average\n",
    "    daily location for each shark, and saves the result to a new CSV.\n",
    "\n",
    "    Args:\n",
    "        input_filename (str): The name of the raw CSV file.\n",
    "        output_filename (str): The name for the new, daily-averaged CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_filename, header=1, on_bad_lines='skip')\n",
    "        print(f\"Successfully loaded '{input_filename}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{input_filename}' not found. Please ensure the file is in the correct directory.\")\n",
    "        return\n",
    "\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # Convert coordinate columns to numeric, handling comma decimals\n",
    "    for col in ['latitude', 'longitude']:\n",
    "        df[col] = df[col].astype(str).str.replace(',', '.')\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Convert date column and drop any rows with invalid data\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df.dropna(subset=['latitude', 'longitude', 'date', 'deployid'], inplace=True)\n",
    "\n",
    "    # Set the 'date' column as the index, which is required for resampling\n",
    "    df = df.set_index('date').sort_index()\n",
    "    print(f\"Initial data cleaned. Starting with {len(df)} valid data points.\")\n",
    "\n",
    "    # We'll process each shark individually\n",
    "    all_sharks_daily = []\n",
    "    \n",
    "    # Group the DataFrame by each unique shark ID\n",
    "    grouped = df.groupby('deployid')\n",
    "    \n",
    "    print(f\"\\nStandardizing data for {len(grouped)} individual sharks by daily average...\")\n",
    "\n",
    "    for shark_id, shark_df in grouped:\n",
    "        # Calculate the mean for numeric columns and take the first entry for categorical ones.\n",
    "        aggregation_rules = {\n",
    "            'latitude': 'mean',\n",
    "            'longitude': 'mean',\n",
    "            'area_tagged': 'first',\n",
    "            'sex': 'first',\n",
    "            'maturity': 'first'\n",
    "        }\n",
    "        \n",
    "        # Apply the aggregation rules while resampling to a daily ('D') interval\n",
    "        daily_df = shark_df.resample('D').agg(aggregation_rules)\n",
    "        \n",
    "        # Add the shark's ID back in, as it's lost during resampling\n",
    "        daily_df['deployid'] = shark_id\n",
    "        \n",
    "        all_sharks_daily.append(daily_df)\n",
    "\n",
    "    # Combine the daily-averaged data for all sharks into a single DataFrame\n",
    "    daily_averaged_df = pd.concat(all_sharks_daily)\n",
    "\n",
    "    # Drop the days where there were no pings (which result in NaN values)\n",
    "    daily_averaged_df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "    \n",
    "    # Reset the index to turn the 'date' back into a regular column\n",
    "    daily_averaged_df.reset_index(inplace=True)\n",
    "\n",
    "    # Save the new, clean dataset to a new CSV file\n",
    "    daily_averaged_df.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete.\")\n",
    "    print(f\"Daily averaged dataset with {len(daily_averaged_df)} data points saved to '{output_filename}'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f37d8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'shark_data.csv'.\n",
      "Initial data cleaned. Starting with 19278 valid data points.\n",
      "\n",
      "Standardizing data for 34 individual sharks by daily average...\n",
      "\n",
      "Preprocessing complete.\n",
      "Daily averaged dataset with 4752 data points saved to 'standardized_shark_data_daily.csv'.\n"
     ]
    }
   ],
   "source": [
    "# --- Execute the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    standardize_shark_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b8938d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
